{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract _Taalportaal_ sentences from XML\n",
    "Hello visitor! Welcome to this notebook! \n",
    "\n",
    "This notebook parses the sentences from Taalportaal into a spreadsheet. The Taalportaal data is in XML format (see the format below). This data is parsed into an excel sheet with five columns: the judgment of the sentence (e.g. '*' or '?'), the sentence itself, the example number, the title and the sourcefile. The last three can help look up the context of a sentence in Taalportaal if necessary. Note that the numbers are given per topic so there are quite a few duplicate numbers in there. \n",
    "\n",
    "The rest of this notebook consists of mostly code. Each time, I have tried to comment the code as carefully as possible to explain what it does. Originally, the goal was to collect sentences that have a ? in their judgment, so the notebook is focused on that. The files that are produced can of course also be used for other purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from xml.dom import minidom\n",
    "import csv\n",
    "import pandas as pd\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure: \n",
    "The Taalportaal data has the general following structure of xml tags: \n",
    "\n",
    "`\n",
    "tp-example\n",
    "   topicid\n",
    "   sourcefile\n",
    "   title\n",
    "   ilexample\n",
    "       sod_ex_index\n",
    "       sod_judgment\n",
    "       sod_bookmarks\n",
    "           sod_bookmark\n",
    "       sod_generalList\n",
    "           innerExample\n",
    "               [sod_ex_index]  (subnumbering, e.g. 1a)\n",
    "               wordgroup\n",
    "                   sod_wg_w\n",
    "                       sod_judgment\n",
    "                       sod_ex_index\n",
    "                       sod_categorialfeature\n",
    "               exampleComment \n",
    "`\n",
    "\n",
    "There is also this alternative structure\n",
    "\n",
    "`\n",
    "tp-example\n",
    "   topicid\n",
    "   sourcefile\n",
    "   title\n",
    "   ilexample\n",
    "       sod_ex_index\n",
    "       sod_judgment\n",
    "       sod_bookmarks\n",
    "           sod_bookmark\n",
    "       sod_generalList\n",
    "           innerExample\n",
    "               [sod_ex_index]  (subnumbering, e.g. 1a)\n",
    "               wordgroup\n",
    "                   lexterm\n",
    "                       word\n",
    "`\n",
    "\n",
    "The problematic thing about the data at hand is that the order of the tags is not used very consistently. This is likely because the file has been collected from the internet. Every example has a different amount of nesting of examples. This makes parsing difficult because we need to account for all possibilities. \n",
    "\n",
    "**NB: It is quite possible that some sentences are not parsed correctly and instead are simply ignored. Feel free to download the code and improve in order to fix it if you want to!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the file we want to parse (assumption that the file has the structure as described above)\n",
    "filename = \"tp_publ_examples_nl_syn_feb22.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the sentence object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    '''\n",
    "    Defines the Sentence object, which has the sentence and judgment as a string\n",
    "    and information on the origin of said sentence (sourcefile, title and example number)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, sentence, judgment, sourcefile, title, ex_number):\n",
    "        '''\n",
    "        Initializes sentence object\n",
    "        '''\n",
    "        self.sentence = sentence\n",
    "        self.judgment = judgment\n",
    "        self.sourcefile = sourcefile\n",
    "        self.title = title\n",
    "        self.ex_number = ex_number\n",
    "    \n",
    "    def has_questionmark_judgment(self):\n",
    "        '''\n",
    "        Returns whether the judgment has a ? in it \n",
    "        Return: boolean\n",
    "        '''\n",
    "        return '?' in self.judgment\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "        Provides string representation for the sentence object\n",
    "        '''\n",
    "        return \"%s %s, which is nr. %s from file %s with title %s\" % (self.judgment, self.sentence, self.ex_number, self.sourcefile, self.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to save data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentences_to_csv(filename, sentences):\n",
    "    '''\n",
    "    writes all sentences to a csv file\n",
    "    filename: the name of the file to be written to\n",
    "    sentences: the sentences to be written down\n",
    "    return: None\n",
    "    '''\n",
    "    with open(filename, 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # write the header\n",
    "        writer.writerow(['judgment','sentence', 'examplenumber', 'title', 'sourcefile'])\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            writer.writerow([sentence.judgment, sentence.sentence, sentence.ex_number, sentence.title, sentence.sourcefile])\n",
    "\n",
    "def write_sentences_to_xlsx(filename, sentences):\n",
    "    '''\n",
    "    writes all sentences to a xlsx file\n",
    "    filename: the name of the file to be written to\n",
    "    sentences: the sentences to be written down\n",
    "    return: None\n",
    "    '''\n",
    "    with open(filename, 'w', encoding='UTF8', newline='') as f:\n",
    "        workbook = xlsxwriter.Workbook(filename)\n",
    "        worksheet = workbook.add_worksheet()\n",
    "\n",
    "               \n",
    "        # write the header\n",
    "        worksheet.write(0, 0, 'judgment')\n",
    "        worksheet.write(0, 1, 'sentence')\n",
    "        worksheet.write(0, 2, 'examplenumber')\n",
    "        worksheet.write(0, 3, 'title')\n",
    "        worksheet.write(0, 4, 'sourcefile')\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            worksheet.write(i+1, 0, sentence.judgment)\n",
    "            worksheet.write(i+1, 1, sentence.sentence)\n",
    "            worksheet.write(i+1, 2, sentence.ex_number)\n",
    "            worksheet.write(i+1, 3, sentence.title)\n",
    "            worksheet.write(i+1, 4, sentence.sourcefile)\n",
    "        \n",
    "        workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing functions\n",
    "This is the actual body of the code, where the magic happens. The main function is `parse_xml_file` and the rest are helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word_to_string(string, word):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    whitespace_like = [' ', '/', '\\n', '\\t']\n",
    "    if len(string) == 0:\n",
    "        string += word\n",
    "    elif string[-1] in whitespace_like:\n",
    "        string += word\n",
    "    else:\n",
    "        string += ' ' + word\n",
    "    return string\n",
    "\n",
    "def add_judgment_to_string(prev_judg, j):\n",
    "    '''\n",
    "    Some sentences contain multiple judgments. This function concatenates a new judgment to previous judgments.\n",
    "    If there are no previous judgments, the new judgment becomes the start of the string. If there were any judgments, \n",
    "    the new judgment is added after adding a '/' to separate the judgments clearly. \n",
    "    \n",
    "    prev_judg: previous judgments\n",
    "    j: new judgment\n",
    "    return: prev_judg + j\n",
    "    '''\n",
    "    if len(prev_judg) == 0:\n",
    "        prev_judg += j\n",
    "    else:\n",
    "        prev_judg += \"/\" + j\n",
    "    return prev_judg\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    '''\n",
    "    Cleans out some of the junk in the parsed sentences. For example, there are \n",
    "    unnecessary enters, tabs, other whitespace, and there are superfluous xml tags\n",
    "    \n",
    "    sentence: sentence to be cleaned (string)\n",
    "    return: cleaned sentence\n",
    "    '''\n",
    "    sentence = sentence.replace('\\n', '')\n",
    "    sentence = sentence.replace('\\t', ' ')\n",
    "    sentence = sentence.replace('  ', ' ')\n",
    "    sentence = sentence.replace('                     ', ' ')\n",
    "    sentence = sentence.replace('    ', ' ')\n",
    "    sentence = sentence.replace('</sod_emphasisitalics><sod_emphasisitalics>', '')\n",
    "    return sentence\n",
    "\n",
    "def append_word(word, sentence, judgment):\n",
    "    if word.nodeType == word.TEXT_NODE:\n",
    "        sentence = add_word_to_string(sentence, word.data.strip())\n",
    "    else:\n",
    "        if word.nodeName == 'sod_judgment':\n",
    "            judgment = add_judgment_to_string(judgment, word.firstChild.data.strip())\n",
    "            sentence = add_word_to_string(sentence, word.firstChild.data.strip())\n",
    "        else:\n",
    "            sentence = add_word_to_string(sentence,  word.toprettyxml())\n",
    "    return sentence, judgment\n",
    "\n",
    "def parse_xml_file(filename):\n",
    "    '''\n",
    "    Parses the xml file under filename\n",
    "    \n",
    "    filename: name of the file to be parsed\n",
    "    return: sentences that have been parsed in the format of a list of Sentence instances\n",
    "    '''\n",
    "    f = minidom.parse(filename)\n",
    "    ex = f.getElementsByTagName('examples')[0]\n",
    "    examples = ex.getElementsByTagName('tp-example')\n",
    "    \n",
    "\n",
    "    problems = 0\n",
    "    sentences = []\n",
    "    \n",
    "    for example in examples:\n",
    "\n",
    "        try: \n",
    "            sourcefile = example.getElementsByTagName('sourcefile')[0].firstChild.data\n",
    "            xml_title = example.getElementsByTagName('title')[0]\n",
    "            title = \"\"\n",
    "            for part in xml_title.childNodes:\n",
    "                try: \n",
    "                    title += part.data\n",
    "                except:\n",
    "                    title += part.toxml()\n",
    "            ilexamples = example.getElementsByTagName('ilexample')\n",
    "            for ilexample in ilexamples:\n",
    "                try: \n",
    "                    il_ex_nr = ilexample.getElementsByTagName('sod_ex_index')[0].firstChild.data\n",
    "                except:\n",
    "                    il_ex_nr = \"\"\n",
    "                sod_generalists = ilexample.getElementsByTagName('sod_generalList')\n",
    "                for sod_generalist in sod_generalists:\n",
    "                    inner_examples = sod_generalist.getElementsByTagName('innerExample')\n",
    "\n",
    "                    for inner_ex in inner_examples:\n",
    "                        try: \n",
    "                            in_ex_nr = inner_ex.getElementsByTagName('sod_ex_index')[0].firstChild.data\n",
    "                        except:\n",
    "                            in_ex_nr = \"\"\n",
    "                        \n",
    "                        #print(inner_ex)\n",
    "                        wordgroups = inner_ex.getElementsByTagName('wordgroup')\n",
    "                        for wordgroup in wordgroups:\n",
    "                            sod_wg_w = wordgroup.getElementsByTagName('sod_wg_w')\n",
    "                            sentence = \"\"\n",
    "                            judgment = \"\"\n",
    "                            for words in sod_wg_w:\n",
    "                                for word in words.childNodes:\n",
    "                                    #print(word.toprettyxml())\n",
    "                                    sentence, judgment = append_word(word, sentence, judgment)\n",
    "                \n",
    "                            lexterms = wordgroup.getElementsByTagName('lexterm')\n",
    "                            for lexterm in lexterms:\n",
    "                                words = wordgroup.getElementsByTagName('word')\n",
    "                                for word in words:\n",
    "                                    sentence = add_word_to_string(sentence, word.toprettyxml())\n",
    "                            sentences.append(Sentence(clean_sentence(sentence), judgment, sourcefile, title, il_ex_nr + in_ex_nr))\n",
    "        except Exception as e:\n",
    "            print(example.toprettyxml())\n",
    "            print(str(e))\n",
    "            problems += 1\n",
    "    if problems>0:\n",
    "        print(\"Encountered: \" + str(problems) + \" problem sentences out of \" + str(len(examples)) + \" sentences\")\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = parse_xml_file(filename)\n",
    "#print(sentences)\n",
    "write_sentences_to_xlsx('sentences.xlsx', sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences(sentences):\n",
    "    '''\n",
    "    Writes only the sentences that contain at least one ? in the judgment to a file\n",
    "    '''\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence.has_questionmark_judgment():\n",
    "            filtered_sentences.append(sentence)\n",
    "    return filtered_sentences\n",
    "\n",
    "filtered_sentences = filter_sentences(sentences)\n",
    "write_sentences_to_xlsx('sentences_filtered.xlsx', filtered_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternotebookenvnew",
   "language": "python",
   "name": "jupyternotebookenvnew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
